# Adversarial AI Saftey Repository

Welcome to the **Adversarial AI Safety** folder. This section focuses on the various types of adversarial attacks that aim to compromise the integrity of machine learning (ML) models, causing misclassifications or poisoning the model's behavior. Understanding these attacks is crucial as we work toward aligning AI with human values and thinking.

## Overview

As we develop AI systems to be more aligned with human values and thinking, it is essential to recognize that these same models are highly vulnerable to adversarial attacks. **Adversarial attacks** exploit weaknesses in ML models, often causing them to misclassify or behave unpredictably. The presence of these vulnerabilities highlights that AI is still far from being aligned with human thought processes, safety standards, and ethical considerations.

This work was inspired by the paper **"Safety‐critical computer vision: an empirical survey of adversarial evasion attacks and defenses on computer vision systems"** by Charles Meyers, Tommy Löfstedt, and Erik Elmroth (2023). The paper covers a variety of adversarial attack methods and defenses for computer vision models. You can read the paper [here](https://link.springer.com/article/10.1007/s10462-023-10521-4).


In this folder, you will find:
- **Jupyter Notebook**: Code that demonstrates adversarial patch attacks on a pre-trained model, visualizes adversarial patches, and evaluates the model's robustness.
- **Patch Images**: Generated adversarial patches that were applied to fool the model during classification tasks.
- **Model Output Logs**: Logs of the model's performance before and after adversarial patches were applied, showcasing accuracy degradation.
- **Research Notes**: Additional conceptual thoughts on adversarial defenses, derived from the experiments and reading of the paper.

## Conclusion

This work is an exploration into how adversarial attacks, particularly **patch attacks**, can affect AI models, and what can be done to defend against them. By understanding these attacks in a hands-on way, we can improve the robustness of AI in critical systems. 
