{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## **Introduction**\n",
        "\n",
        "This notebook demonstrates how to use BERT for toxic content detection and leverage LIME (Local Interpretable Model-Agnostic Explanations) to explain the predictions made by the BERT model. We will walk through the process of classifying toxic content in text using a pre-trained BERT model and then visualize which words contributed the most to the model's decision using LIME."
      ],
      "metadata": {
        "id": "hpRX__Du8zWY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "Author: Lennox Anderson\n",
        "\n",
        "Date Modified: September 29th, 2024."
      ],
      "metadata": {
        "id": "180GoBCjDBjS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "NcT0CfMe-l2n"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rF9oOhcL4nrb"
      },
      "source": [
        "## **Dependencies**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "O6aoQCCGtCmK",
        "outputId": "378a5599-a974-47e6-f188-e4a087e1ef4b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: shap in /usr/local/lib/python3.10/dist-packages (0.46.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from shap) (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from shap) (1.13.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from shap) (1.5.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from shap) (2.1.4)\n",
            "Requirement already satisfied: tqdm>=4.27.0 in /usr/local/lib/python3.10/dist-packages (from shap) (4.66.5)\n",
            "Requirement already satisfied: packaging>20.9 in /usr/local/lib/python3.10/dist-packages (from shap) (24.1)\n",
            "Requirement already satisfied: slicer==0.0.8 in /usr/local/lib/python3.10/dist-packages (from shap) (0.0.8)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from shap) (0.60.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from shap) (2.2.1)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->shap) (0.43.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->shap) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->shap) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->shap) (2024.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->shap) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->shap) (3.5.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->shap) (1.16.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.24.7)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.6.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: lime in /usr/local/lib/python3.10/dist-packages (0.2.0.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from lime) (3.7.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from lime) (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from lime) (1.13.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from lime) (4.66.5)\n",
            "Requirement already satisfied: scikit-learn>=0.18 in /usr/local/lib/python3.10/dist-packages (from lime) (1.5.2)\n",
            "Requirement already satisfied: scikit-image>=0.12 in /usr/local/lib/python3.10/dist-packages (from lime) (0.24.0)\n",
            "Requirement already satisfied: networkx>=2.8 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.12->lime) (3.3)\n",
            "Requirement already satisfied: pillow>=9.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.12->lime) (10.4.0)\n",
            "Requirement already satisfied: imageio>=2.33 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.12->lime) (2.35.1)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.12->lime) (2024.9.20)\n",
            "Requirement already satisfied: packaging>=21 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.12->lime) (24.1)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.10/dist-packages (from scikit-image>=0.12->lime) (0.4)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.18->lime) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.18->lime) (3.5.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lime) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lime) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lime) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lime) (1.4.7)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lime) (3.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->lime) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->lime) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install shap\n",
        "!pip install transformers\n",
        "!pip install lime"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Section 1: Toxic Content Detection using BERT**\n",
        "\n",
        "In this section, we load a pre-trained BERT model that is fine-tuned for toxic content detection. The BERT model will predict categories such as toxic, severe toxic, obscene, threat, insult, and identity hate based on the input text."
      ],
      "metadata": {
        "id": "4gqtRUje-nVa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"You are a dumb asshole\""
      ],
      "metadata": {
        "id": "ISK1qq49Cle_"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qNp3eEq2ygPv",
        "outputId": "6104fa66-8304-429e-efed-99690c316132"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted class: toxic\n",
            "Class probabilities: [[7.3768127e-01 8.5911941e-04 1.0318284e-01 7.2450780e-06 1.5816484e-01\n",
            "  1.0469716e-04]]\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "import warnings\n",
        "\n",
        "# handle warnings\n",
        "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
        "\n",
        "# initialize BERT model toxcity detection and tokenizing\n",
        "model_name = \"unitary/toxic-bert\"\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "model = BertForSequenceClassification.from_pretrained(model_name)\n",
        "model.eval()\n",
        "\n",
        "# tokenize and convert to tensors\n",
        "# we convert the tokenizer output to tensors because tensors are the format BERT requires for processing\n",
        "inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True, max_length=512)\n",
        "\n",
        "# speed up the process since gradients are not needed for making predictions\n",
        "# pass the tokenized tensors to BERT to generate predictions\n",
        "# Extracts the raw, unnormalized output scores (logits) from the model.\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "    logits = outputs.logits\n",
        "\n",
        "# apply softmax\n",
        "# softmax: a mathematical function that converts raw scores (logits) into probabilities, ensuring that the sum of the probabilities is 1\n",
        "probabilities = torch.softmax(logits, dim=-1).numpy()\n",
        "\n",
        "# labels from BERT\n",
        "class_names = ['toxic', 'severe toxic', 'obscene', 'threat', 'insult', 'identity hate']\n",
        "\n",
        "predicted_class = class_names[probabilities.argmax()]\n",
        "print(f\"Predicted class: {predicted_class}\")\n",
        "print(f\"Class probabilities: {probabilities}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Section 2: Explaining Model Predictions with LIME**\n",
        "\n",
        "LIME helps make model predictions interpretable by showing which parts of the text (words or phrases) contributed most to the final classification. In this section, we use LIME to generate an explanation for the BERT model's toxic content prediction."
      ],
      "metadata": {
        "id": "yT8_DjQX-zD2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"you are a great big beautiful bitch\""
      ],
      "metadata": {
        "id": "TgMKQfj6Phzs"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pFM-BUfBsuwx"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import BertTokenizer, BertForSequenceClassification\n",
        "import lime\n",
        "from lime.lime_text import LimeTextExplainer\n",
        "import numpy as np\n",
        "\n",
        "# initialize BERT model for toxicity detection and tokenizing\n",
        "model_name = \"unitary/toxic-bert\"\n",
        "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
        "model = BertForSequenceClassification.from_pretrained(model_name)\n",
        "model.eval()\n",
        "\n",
        "# BERTs labels\n",
        "class_names = ['toxic', 'severe toxic', 'obscene', 'threat', 'insult', 'identity hate']\n",
        "\n",
        "inputs = tokenizer(text, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "    logits = outputs.logits\n",
        "    probabilities = torch.softmax(logits, dim=-1).numpy()\n",
        "\n",
        "# initialize LIME text explainer\n",
        "explainer = LimeTextExplainer(class_names=class_names)\n",
        "\n",
        "# perturbs the text for LIME analysis.\n",
        "# gets model probabilities using softmax.\n",
        "# highlights important words affecting predictions.\n",
        "explanation = explainer.explain_instance(text, lambda x: torch.softmax(model(**tokenizer(x, return_tensors='pt', padding=True, truncation=True, max_length=512)).logits, dim=-1).detach().numpy(), num_features=10)\n",
        "\n",
        "# show bar chart explanation\n",
        "explanation.show_in_notebook(text=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DPXufMOVtXqA"
      },
      "outputs": [],
      "source": [
        "# raw explaination\n",
        "explanation_list = explanation.as_list()\n",
        "for feature, weight in explanation_list:\n",
        "    print(f\"Feature: {feature}, Weight: {weight}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eZu1rPKJ_H1D"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}